{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Javeriana.svg/1200px-Javeriana.svg.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "# Facultad de Ingeniería Industrial\n",
        "## Maestria en Analítica en Inteligencia de Negocio\n",
        "### Materia: Tópicos Avanzados en Analítica\n",
        "### Modúlo: Procesamiento de Lenguaje Natural"
      ],
      "metadata": {
        "id": "R36i0tfEz4Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenización\n",
        "\n",
        "En NLP el proceso de convertir nuestras secuencias de caracteres, palabras o párrafos en inputs para la computadora se llama **tokenización**. Se puede pensar al token como la unidad para procesamiento semántico."
      ],
      "metadata": {
        "id": "Nfb0e7Js1dEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv7vRwTzzXvH",
        "outputId": "7fa4e32f-aa71-4514-82c1-aaa667e22593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado = tk.tokenize(texto)\n",
        "print(texto_tokenizado)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "como vemos *manzana* y *cuánto* figuran con el signo de pregunta. Y si tuvieramos la palabra manzana mencionada otra vez saldría nuevamente como un token diferente. Lo mismo nos sucedería si aparece una coma o un punto ¿Cómo hacemos para evitarlo?"
      ],
      "metadata": {
        "id": "FbM9INzk3HWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos utilizar `TreebankWordTokenizer` en lugar de `WhitespaceTokenizer`. También podemos preprocesar el texto quitando comas y signos de puntuación y separar por espacios, o bien utilizar opciones como `WordPunctTokenizer` que separa por palabras tomando como separadores todo lo que no sea un caracter alfabetico."
      ],
      "metadata": {
        "id": "pR4uGVu73kgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
        "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woH8Vq5W1bwp",
        "outputId": "cd55be02-9f9a-4d2c-971d-c53603f6b587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿', 'Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n",
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6D2WNuV18eX",
        "outputId": "2d975450-e427-4f41-9476-f573dc6d2d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿', 'Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAMX05Gx1_qW",
        "outputId": "de4e10b0-183d-4cf6-f65d-840df669f1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como evidenciamos la opción de `TreebankWordTokenizer` es la más popular en inglés el signo de apertura de pregunta \"¿\" fue un problema para ella. Mientras que la opción de WordPunctTokenizer no tuvo ningún problema."
      ],
      "metadata": {
        "id": "Pi0PPMN038Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿cómo crees que se comportarían frente a una apóstrofe?"
      ],
      "metadata": {
        "id": "NpqVpFFY4gxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
        "\n",
        "texto = \"My girlfriend's house is bigger than mine\"\n",
        "\n",
        "# Tokenización con WordPunctTokenizer\n",
        "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
        "\n",
        "# Tokenización con TreebankWordTokenizer\n",
        "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)\n",
        "\n",
        "print(\"Tokenización con WordPunctTokenizer:\")\n",
        "print(texto_tokenizado1)\n",
        "print(\"\\nTokenización con TreebankWordTokenizer:\")\n",
        "print(texto_tokenizado2)\n",
        "\n",
        "# En este caso WordPuncTokenizer no tuvo problema y separó el apostrofe completamente de las palabras o de la letra s\n",
        "# mientras que Treebank... mantuvo el apostrofe con la letra ('s)\n"
      ],
      "metadata": {
        "id": "i3LHqWjY2ATa",
        "outputId": "a95de8a7-8695-422a-bb5a-65908623baef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenización con WordPunctTokenizer:\n",
            "['My', 'girlfriend', \"'\", 's', 'house', 'is', 'bigger', 'than', 'mine']\n",
            "\n",
            "Tokenización con TreebankWordTokenizer:\n",
            "['My', 'girlfriend', \"'s\", 'house', 'is', 'bigger', 'than', 'mine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-HCpF8SvPYC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}